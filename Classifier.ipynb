{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
    "# Classification of documents in official daily juridical processes of state of São Paulo: \n",
    "### Example of a official daily of 2 jun. 2020\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"assets/officialDailySP.png\" width=900 height=700> \n",
    "\n",
    "    \n",
    "    In this project we want to classify the type of a content of a document present in a official daily\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    In simple words we want to classify the jucicial processes extract from official daily of period in 2015 to 2017, in sentences (processes which the judge finish the process) and documents which are not sentences.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "### 1. Reading the data, library import, and seeying our data:\n",
    "<!-- <blockquote>\n",
    "csv, xlsx and json data formats\n",
    "</blockquote> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# sp_data is a pandas dataframe of our data\n",
    "sp_data = pd.read_json('data/Amostra100KFiltrada.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.local/lib/python3.6/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# seeying our data\n",
    "count_separator = [] # this is a vector which contains the frequency of ' - ' in each Document\n",
    "\n",
    "count_Counteudo_length = [] # this is a vector which contains the lenght of \"Conteúdo\" in each Document\n",
    "\n",
    "# for each Document in data, count the frequency of ' - ' and the len of \"Conteúdo\"\n",
    "for i, Document in enumerate(sp_data['Conteúdo']):\n",
    "    count_separator.append(Document.count(' - '))\n",
    "\n",
    "data = np.array(count_separator) # vector which contains the amount of ' - ' in each document\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# FILTER THE CONTEUDO OF NOT SENTENCES DOCUMENTS WHICH HAS 4 OR MORE ' - '\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# regex for search of a subcontent of sentences in our data\n",
    "# regex = r\"( julgo | homologo.*o acordo)\" # deu 97%??? deve ta errado\n",
    "regex = r\"(?i)(.JULGO PROCEDENTE.[^EM PARTE]|.JULGO TOTALMENTE PROCEDENTE.|.JULGO PROCEDENTE EM PARTE.|.JULGO PARCIALMENTE PROCEDENTE.|.JULGO IMPROCEDENTE.|.JULGO TOTALMENTE IMPROCEDENTE.|.JULGO EXTINTO.*sem.(julgamento|resolução).de.mérito.|.hom(o|ó)logo o acordo.|.(art\\.?|artigo) 284.)\"\n",
    "Doc_series = pd.Series(sp_data['Conteúdo']) # Transform the pandas Data.frame to a series for initial interting spot\n",
    "\n",
    "# Filter the documents which has the regex in \"conteúdo\"\n",
    "sentencas = Doc_series.str.contains(regex, case=False)\n",
    "sentencas = sentencas.to_numpy()\n",
    "\n",
    "ind_range = (data > 4) & (sentencas == False)\n",
    "\n",
    "data_4_more = sp_data['Conteúdo'][ind_range]\n",
    "\n",
    "positions = []\n",
    "\n",
    "processos = []\n",
    "\n",
    "classes = []\n",
    "\n",
    "assuntos = []\n",
    "\n",
    "partes = []\n",
    "\n",
    "conteudo = []\n",
    "\n",
    "advs = []\n",
    "\n",
    "for document in data_4_more:\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the processes\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    position = re.search(r' - ', document)\n",
    "    processos.append(document[0:position.start()])\n",
    "\n",
    "    doc_no_processos = document[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the classes\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_processos)\n",
    "    classes.append(doc_no_processos[0:position.start()])\n",
    "\n",
    "    doc_no_classes_to = doc_no_processos[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the assuntos\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_classes_to)\n",
    "    assuntos.append(doc_no_classes_to[0:position.start()])\n",
    "\n",
    "    doc_no_assuntos_to = doc_no_classes_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the parts\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_assuntos_to)\n",
    "    partes.append(doc_no_assuntos_to[0:position.start()])\n",
    "\n",
    "    doc_no_parts_to = doc_no_assuntos_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the content\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the ADV\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r\"(?i)- ADV\", doc_no_parts_to)\n",
    "    if position:\n",
    "        advs.append(doc_no_parts_to[position.start():])\n",
    "\n",
    "        conteudo.append(doc_no_parts_to[0: position.start()])\n",
    "    else:\n",
    "        conteudo.append(doc_no_parts_to)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "len(data_4_more)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# FILTER THE CONTEUDO OF SENTENCES DOCUMENTS WHICH HAS 4 OR MORE ' - '\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "C_sentencas = sp_data[sentencas]['Conteúdo']\n",
    "cont_sentencas = data[sentencas]\n",
    "\n",
    "more = cont_sentencas > 4\n",
    "\n",
    "teste = C_sentencas[more]\n",
    "\n",
    "processos_sentencas = []\n",
    "\n",
    "classes_sentencas = []\n",
    "\n",
    "assuntos_sentencas = []\n",
    "\n",
    "partes_sentencas = []\n",
    "\n",
    "conteudo_sentencas = []\n",
    "\n",
    "advs_sentencas = []\n",
    "\n",
    "for i, document in enumerate(teste):\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the processes\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    position = re.search(r' - ', document)\n",
    "    if position:\n",
    "        processos_sentencas.insert(i, document[0:position.start()])\n",
    "\n",
    "        doc_no_processos = document[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the classes\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_processos)\n",
    "\n",
    "    if position:\n",
    "        classes_sentencas.insert(i, doc_no_processos[0:position.start()])\n",
    "\n",
    "        doc_no_classes_to = doc_no_processos[position.end():]\n",
    "    else:\n",
    "        conteudo_sentencas.insert(i, doc_no_processos)\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the assuntos\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_classes_to)\n",
    "    if position:\n",
    "        assuntos_sentencas.insert(i, doc_no_classes_to[0:position.start()])\n",
    "\n",
    "        doc_no_assuntos_to = doc_no_classes_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the parts\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_assuntos_to)\n",
    "    if position:\n",
    "        partes_sentencas.insert(i, doc_no_assuntos_to[0:position.start()])\n",
    "\n",
    "        doc_no_parts_to = doc_no_assuntos_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the content\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the ADV\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r\"(?i)- ADV\", doc_no_parts_to)\n",
    "    if position:\n",
    "        advs_sentencas.insert(i, doc_no_parts_to[position.start():])\n",
    "\n",
    "        conteudo_sentencas.insert(i, doc_no_parts_to[0: position.start()])\n",
    "    else:\n",
    "        conteudo_sentencas.insert(i, doc_no_parts_to)\n",
    "\n",
    "len(conteudo_sentencas)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# TOKENIZER THE \"CONTEUDO\" OF SENTENCES \n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer(num_words=100)\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(conteudo_sentencas)\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# GETTING THE TRAIN AND TEST DATASETS \n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "series = pd.Series(conteudo_sentencas)\n",
    "not_sentence = pd.Series(conteudo)\n",
    "\n",
    "X_train = pd.Series(conteudo_sentencas)\n",
    "not_sentence_train = not_sentence.sample(len(X_train), random_state=123)\n",
    "\n",
    "Y_train = [1] * len(X_train)\n",
    "Y_sentence_train = [0] * len(X_train)\n",
    "\n",
    "X_train = pd.concat([X_train, not_sentence_train])\n",
    "Y_train = Y_train + Y_sentence_train\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "   X_train, Y_train, test_size=0.2, random_state=100)\n",
    "\n",
    "# X_train = t.texts_to_matrix(sentences_train, mode='count')\n",
    "# X_teste = t.texts_to_matrix(sentences_test, mode='count')\n",
    "\n",
    "# X_train = t.texts_to_matrix(sentences_train, mode='freq')\n",
    "# X_teste = t.texts_to_matrix(sentences_test, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [i for i in range(len(Y_train))]\n",
    "df = pd.DataFrame({'Content': X_train, 'Class': Y_train})\n",
    "df.index = ind\n",
    "df.to_csv('SentencesAndNotSentences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banco Itauleasing S/A - Diante do exposto e de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telefônica Brasil S/A - Em execução individual...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONCLUSÃO Em 22 de outubro de 2012 faço estes ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uma vez que o autor quedou-se inerte quanto à ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bradesco - Banco Brasileiro de Descontos S/a, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>Seguradora Líder dos Consórcios DPVAT - Fls. 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>Justiça Pública - Anderson Reginaldo Rizzo e o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>Antonio Silva dos Santos - Vistos.Fl. 71. Inde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>UNIMED DE GUARULHOS COOPERATIVA DE TRABALHO ME...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Fls. 63 - Vistos. O exequente deverá proceder ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9992 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content  Class\n",
       "0     Banco Itauleasing S/A - Diante do exposto e de...      1\n",
       "1     Telefônica Brasil S/A - Em execução individual...      1\n",
       "2     CONCLUSÃO Em 22 de outubro de 2012 faço estes ...      1\n",
       "3     Uma vez que o autor quedou-se inerte quanto à ...      1\n",
       "4     Bradesco - Banco Brasileiro de Descontos S/a, ...      1\n",
       "...                                                 ...    ...\n",
       "9987  Seguradora Líder dos Consórcios DPVAT - Fls. 2...      0\n",
       "9988  Justiça Pública - Anderson Reginaldo Rizzo e o...      0\n",
       "9989  Antonio Silva dos Santos - Vistos.Fl. 71. Inde...      0\n",
       "9990  UNIMED DE GUARULHOS COOPERATIVA DE TRABALHO ME...      0\n",
       "9991  Fls. 63 - Vistos. O exequente deverá proceder ...      0\n",
       "\n",
       "[9992 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('SentencesAndNotSentences.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-env",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
